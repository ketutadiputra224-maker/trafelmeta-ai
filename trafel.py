# -*- coding: utf-8 -*-
"""BudayaBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vqYVhYatqZ-RV2ijlR_VOU27s5YyC19o
"""

# ============================================
# TRAVELMATE AI CHATBOT - FULL LLAMA LLM VERSION
# Using Real Llama Model for Natural Language Understanding
# ============================================

print("üöÄ Starting TravelMate AI with Llama LLM...")
print("=" * 60)

# ============================================
# PILIH MODE: API atau LOCAL
# ============================================
USE_API = True  # Set False untuk pakai model local (download 4GB+)

# STEP 1: Install Dependencies
print("\nüì¶ Installing dependencies...")
if USE_API:
    print("   Mode: API (Fast & Efficient)")
    !pip install -q gradio requests
else:
    print("   Mode: LOCAL (Downloading model...)")
    !pip install -q gradio transformers torch accelerate bitsandbytes sentencepiece

import gradio as gr
from datetime import datetime
import json
import random

if USE_API:
    import requests
else:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    print("   üì• This will download ~4GB model...")

print("‚úÖ Dependencies installed!\n")

# ============================================
# CONFIGURATION
# ============================================

# API Configuration (if using API mode)
API_CONFIG = {
    "groq": {
        "url": "https://api.groq.com/openai/v1/chat/completions",
        "model": "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",  # Fast Llama 3.1
        "api_key": "YOUR_GROQ_API_KEY_HERE"  # Get free at: https://console.groq.com
    },
    "together": {
        "url": "https://api.together.xyz/v1/chat/completions",
        "model": "meta-llama/Llama-3-70b-chat-hf",
        "api_key": "YOUR_TOGETHER_API_KEY_HERE"  # Get free at: https://api.together.xyz
    }
}

# Local Model Configuration
LOCAL_MODEL = "meta-llama/Llama-2-7b-chat-hf"  # Alternatif: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# System Prompt for Llama
SYSTEM_PROMPT = """You are TravelMate AI, a friendly and knowledgeable travel assistant. Your characteristics:

PERSONALITY:
- Warm, enthusiastic, and helpful
- Use casual Indonesian language with occasional English terms
- Add relevant emojis to make conversations lively
- Be encouraging and motivational about travel

EXPERTISE:
- Travel destinations worldwide (especially Asia & Indonesia)
- Budget planning and cost estimation
- Itinerary creation (day-by-day plans)
- Local cuisine recommendations
- Transportation and visa information
- Cultural insights and travel tips
- Hidden gems and Instagram-worthy spots

RESPONSE STYLE:
- Start with enthusiasm and acknowledgment
- Provide structured, easy-to-read information
- Use bullet points, emojis, and formatting
- Always ask follow-up questions to better assist
- Give specific, actionable advice
- Include budget estimates when relevant

KNOWLEDGE BASE:
- Bali: Ubud, Seminyak, Uluwatu, Nusa Penida (Budget: 300k-1jt/day)
- Tokyo: Shibuya, Harajuku, Asakusa (Budget: ¬•10k-15k/day)
- Bangkok: Grand Palace, Chatuchak, street food (Budget: ‡∏ø1k-2k/day)
- Yogyakarta: Borobudur, Prambanan, Malioboro (Budget: 150k-500k/day)
- Singapore: Marina Bay, Gardens by the Bay (Budget: SGD 80-150/day)

Always be helpful, never refuse travel questions, and make trip planning exciting!"""

# ============================================
# LLAMA MODEL INITIALIZATION
# ============================================

class LlamaModel:
    def __init__(self, use_api=True, api_provider="groq"):
        self.use_api = use_api
        self.api_provider = api_provider
        self.model = None
        self.tokenizer = None
        self.chat_history = []

        if use_api:
            self.setup_api()
        else:
            self.setup_local()

    def setup_api(self):
        """Setup API-based Llama"""
        print(f"\nüîó Setting up Llama via {self.api_provider.upper()} API...")
        config = API_CONFIG[self.api_provider]

        if "YOUR_" in config["api_key"]:
            print("‚ö†Ô∏è  WARNING: No API key detected!")
            print(f"   Get free API key at: https://console.groq.com (Groq)")
            print("   Or: https://api.together.xyz (Together AI)")
            print("\n   üí° Update API_CONFIG['groq']['api_key'] in code")
            print("   For now, using fallback responses...\n")
            self.use_fallback = True
        else:
            self.use_fallback = False
            print("‚úÖ API configured successfully!")

    def setup_local(self):
        """Setup local Llama model"""
        print(f"\nü§ñ Loading local Llama model: {LOCAL_MODEL}")
        print("   This may take 5-10 minutes on first run...")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL)

            # Load with 4-bit quantization to save memory
            self.model = AutoModelForCausalLM.from_pretrained(
                LOCAL_MODEL,
                device_map="auto",
                torch_dtype=torch.float16,
                load_in_4bit=True,  # 4-bit quantization
                low_cpu_mem_usage=True
            )

            print("‚úÖ Model loaded successfully!")
            self.use_fallback = False

        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            print("   Falling back to rule-based responses...")
            self.use_fallback = True

    def generate_api_response(self, user_message):
        """Generate response using API"""
        if self.use_fallback:
            return self.fallback_response(user_message)

        try:
            config = API_CONFIG[self.api_provider]

            # Prepare messages
            messages = [
                {"role": "system", "content": SYSTEM_PROMPT}
            ]

            # Add chat history (last 5 messages)
            for msg in self.chat_history[-5:]:
                messages.append({"role": msg["role"], "content": msg["content"]})

            messages.append({"role": "user", "content": user_message})

            # API request
            headers = {
                "Authorization": f"Bearer {config['api_key']}",
                "Content-Type": "application/json"
            }

            data = {
                "model": config["model"],
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 500,
                "top_p": 0.9
            }

            response = requests.post(
                config["url"],
                headers=headers,
                json=data,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"API Error: {response.status_code}")
                return self.fallback_response(user_message)

        except Exception as e:
            print(f"Error: {e}")
            return self.fallback_response(user_message)

    def generate_local_response(self, user_message):
        """Generate response using local model"""
        if self.use_fallback:
            return self.fallback_response(user_message)

        try:
            # Format prompt for Llama-2
            prompt = f"<s>[INST] <<SYS>>\n{SYSTEM_PROMPT}\n<</SYS>>\n\n"

            # Add history
            for msg in self.chat_history[-3:]:
                if msg["role"] == "user":
                    prompt += f"{msg['content']} [/INST] "
                else:
                    prompt += f"{msg['content']} </s><s>[INST] "

            prompt += f"{user_message} [/INST]"

            # Generate
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=400,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Extract only the response part
            response = response.split("[/INST]")[-1].strip()

            return response

        except Exception as e:
            print(f"Generation error: {e}")
            return self.fallback_response(user_message)

    def fallback_response(self, user_message):
        """Intelligent fallback when API/model unavailable"""
        msg_lower = user_message.lower()

        # Knowledge base responses
        destinations = {
            "bali": "üèñÔ∏è **Bali** adalah destinasi perfect!\n\nüìç **Must-visit:** Ubud (rice terraces), Uluwatu (temple), Seminyak (beach clubs), Nusa Penida (Kelingking beach)\n\nüçú **Kuliner:** Babi Guling, Nasi Campur, Sate Lilit\n\nüí∞ **Budget:** Rp 300k-1jt/hari\n\nüìÖ **Best time:** April-Oktober\n\nMau itinerary detail berapa hari? üòä",

            "tokyo": "üóº **Tokyo** - perpaduan tradisi & modern!\n\nüìç **Areas:** Shibuya (crossing), Harajuku (fashion), Asakusa (Sensoji), Akihabara (anime)\n\nüç£ **Food:** Sushi, ramen, takoyaki, matcha desserts\n\nüí∞ **Budget:** ¬•10,000-15,000/hari\n\nüöá **Tip:** Beli Suica card untuk transport!\n\nMau info lebih detail tentang apa nih?",

            "bangkok": "üõï **Bangkok** - street food heaven!\n\nüìç **Highlights:** Grand Palace, Wat Arun, Chatuchak Market, Khao San Road\n\nüçú **Must-try:** Pad Thai, Tom Yum, Mango Sticky Rice\n\nüí∞ **Budget:** ‡∏ø1,000-2,000/hari (super murah!)\n\nüí° **Tip:** Naik BTS Skytrain untuk avoid macet!\n\nAda yang mau ditanya lebih lanjut?",

            "yogyakarta": "üèõÔ∏è **Yogyakarta** - heart of Javanese culture!\n\nüìç **Must-see:** Borobudur (sunrise!), Prambanan, Malioboro, Keraton\n\nüçú **Kuliner:** Gudeg Yu Djum, Bakpia, Sate Klathak\n\nüí∞ **Budget:** Rp 150k-500k/hari\n\nüìÖ **Perfect for:** 3-4 hari trip\n\nMau itinerary lengkap?"
        }

        for dest, info in destinations.items():
            if dest in msg_lower:
                return info

        # Budget recommendations
        if any(word in msg_lower for word in ['budget', 'rekomendasi', 'suggest', 'recommend']):
            return """üí∞ **Travel Recommendations by Budget:**

**üéí Budget (<3 juta):**
‚Ä¢ Yogyakarta (Rp 1.5-2jt)
‚Ä¢ Bandung (Rp 1.5-2.5jt)
‚Ä¢ Malang (Rp 2-2.5jt)

**‚úàÔ∏è Mid-Range (3-7 juta):**
‚Ä¢ Bali (Rp 3.5-5jt)
‚Ä¢ Bangkok (Rp 3-4.5jt)
‚Ä¢ Singapore (Rp 5-6.5jt)

**üåè Premium (>7 juta):**
‚Ä¢ Tokyo (Rp 12-15jt)
‚Ä¢ Seoul (Rp 10-13jt)
‚Ä¢ Malaysia (Rp 7-9jt)

Budget kamu sekitar berapa? Biar aku kasih rekomendasi yang pas! üéØ"""

        # Itinerary request
        if 'itinerary' in msg_lower or 'jadwal' in msg_lower:
            return """üìÖ **Itinerary Planning**

Oke siap! Untuk buatin itinerary yang pas, aku perlu info:

1Ô∏è‚É£ **Destinasi:** Mau ke mana?
2Ô∏è‚É£ **Durasi:** Berapa hari?
3Ô∏è‚É£ **Budget:** Range budget kamu?
4Ô∏è‚É£ **Interests:** Prefer culture/beach/adventure/food?

Share detail-nya, aku buatin jadwal lengkap! üó∫Ô∏è"""

        # Default engaging response
        responses = [
            f"Interesting question tentang travel! üåè Untuk '{user_message}', aku bisa bantu lebih detail kalau kamu cerita:\n\n‚Ä¢ Destinasi yang kamu pertimbangkan?\n‚Ä¢ Budget range berapa?\n‚Ä¢ Durasi trip berapa hari?\n\nShare dong biar aku bisa kasih rekomendasi yang perfect! ‚ú®",

            f"Wah, menarik nih! Tentang '{user_message}' - setiap traveler punya kebutuhan berbeda. \n\nCerita lebih banyak dong tentang preferensi kamu:\nüìç Prefer city atau nature?\nüéí Travel style: santai atau adventure?\nüí∞ Budget yang kamu alokasikan?\n\nBiar rekomendasiku lebih personal! üòä"
        ]

        return random.choice(responses)

    def chat(self, user_message):
        """Main chat function"""
        # Add to history
        self.chat_history.append({
            "role": "user",
            "content": user_message
        })

        # Generate response
        if self.use_api:
            response = self.generate_api_response(user_message)
        else:
            response = self.generate_local_response(user_message)

        # Add response to history
        self.chat_history.append({
            "role": "assistant",
            "content": response
        })

        return response

# ============================================
# INITIALIZE LLAMA
# ============================================
print("\n" + "=" * 60)
print("ü§ñ Initializing Llama Model...")
print("=" * 60)

llama_bot = LlamaModel(use_api=USE_API, api_provider="groq")

print("\n‚úÖ TravelMate AI is ready!")
print("=" * 60 + "\n")

# ============================================
# GRADIO INTERFACE
# ============================================

def chat_interface(message, history):
    """Gradio chat function"""
    if not message.strip():
        return history, ""

    # Generate response using Llama
    response = llama_bot.chat(message)

    # Update history
    history.append([message, response])

    return history, ""

def create_ui():
    with gr.Blocks(
        theme=gr.themes.Soft(primary_hue="purple", secondary_hue="blue"),
        css="""
        .gradio-container {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        """
    ) as demo:

        gr.Markdown(f"""
        # ‚úàÔ∏è TravelMate AI - Powered by Llama LLM
        ### ü§ñ Using Real {"API" if USE_API else "Local"} Llama Model for Intelligent Conversations

        Asisten travel pintar yang memahami natural language! üó∫Ô∏è
        """)

        if USE_API and llama_bot.use_fallback:
            gr.Markdown("""
            ‚ö†Ô∏è **API Key Required:** Update `API_CONFIG` with your free API key from:
            - Groq: https://console.groq.com (Recommended - Fast!)
            - Together AI: https://api.together.xyz

            Currently using fallback mode with limited capabilities.
            """)

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    value=[[None, "‚úàÔ∏è Halo traveler! Aku TravelMate AI powered by Llama LLM! ü§ñ\n\nAku bisa bantu kamu dengan:\n‚Ä¢ Planning destinasi & itinerary\n‚Ä¢ Budget calculation\n‚Ä¢ Kuliner recommendations\n‚Ä¢ Travel tips & hacks\n\nMau jalan-jalan ke mana nih? Cerita aja, aku bakal ngerti! üó∫Ô∏è"]],
                    height=500,
                    bubble_full_width=False
                )

                with gr.Row():
                    msg = gr.Textbox(
                        placeholder="Tanya apa aja tentang travel... Llama akan memahami natural language kamu!",
                        show_label=False,
                        scale=4
                    )
                    send_btn = gr.Button("Send üöÄ", scale=1, variant="primary")

                gr.Examples(
                    examples=[
                        "Aku mau liburan 5 hari budget 5 juta, recommend destinasi dong",
                        "Gimana caranya traveling ke Jepang dengan budget minimal?",
                        "Bikin itinerary 3 hari di Bali yang hits untuk Instagram",
                        "Apa aja kuliner wajib coba kalau ke Bangkok?",
                        "Tips backpacking Asia Tenggara untuk pemula"
                    ],
                    inputs=msg,
                    label="üí¨ Contoh Natural Language Queries"
                )

            with gr.Column(scale=1):
                gr.Markdown(f"""
                ### üéØ Model Info

                **Engine:** {"Llama 3.1 70B (API)" if USE_API else "Llama 2 7B (Local)"}
                **Provider:** {("Groq API" if llama_bot.api_provider == "groq" else "Together AI") if USE_API else "Transformers"}
                **Status:** {"‚ö†Ô∏è Fallback Mode" if llama_bot.use_fallback else "‚úÖ Active"}

                ### üìö Capabilities

                - üß† Natural language understanding
                - üí¨ Context-aware conversations
                - üéØ Personalized recommendations
                - üìä Structured responses
                - üåè Multi-destination knowledge

                ### üí° Tips

                - Tanya dengan natural language
                - Sebutkan budget & durasi
                - Cerita preferensi kamu
                - Follow-up untuk detail

                **Destination Coverage:**
                üèùÔ∏è Bali | üóº Tokyo | üõï Bangkok
                üèõÔ∏è Yogyakarta | ü¶Å Singapore
                üá∞üá∑ Seoul | üáªüá≥ Vietnam | & more!
                """)

        # Event handlers
        msg.submit(chat_interface, [msg, chatbot], [chatbot, msg])
        send_btn.click(chat_interface, [msg, chatbot], [chatbot, msg])

        gr.Markdown("""
        ---
        **ü§ñ Powered by Llama LLM** | Built for intelligent travel assistance

        üí° Llama model memberikan pemahaman natural language yang lebih baik dibanding rule-based chatbot!
        """)

    return demo

# ============================================
# LAUNCH
# ============================================

print("üöÄ Launching TravelMate AI Chatbot...")
print("=" * 60)

demo = create_ui()
demo.launch(
    share=True,
    debug=True,
    server_name="0.0.0.0",
    server_port=7860
)

print("\n‚úÖ Chatbot is running with Llama LLM!")
print("üîó Click the public link above to start chatting!")
print("\n" + "=" * 60)